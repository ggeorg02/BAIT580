{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e66709ec",
   "metadata": {},
   "source": [
    "# Lecture 6: (de)Normalization & Data Warehousing\n",
    "Gittu George, January 20 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a48923",
   "metadata": {},
   "source": [
    "## Todays Agenda\n",
    "- Quick Recap\n",
    "- WHAT is a Data Warehouse?\n",
    "- WHY Data Warehouse? \n",
    "- HOW do we build a data warehouse?\n",
    "- Differences\n",
    "- Wrap up\n",
    "\n",
    "## Learning objectives\n",
    "- Understanding of data warehouse.\n",
    "- Building a data warehouse.\n",
    "- Connecting our learning from previous classes.\n",
    "\n",
    "## Quick Recap \n",
    "- Building workflows to generate actionable insights using data\n",
    "- How to take raw data from independent sources and load it to the database\n",
    "- How to take the data and manage it within a database, using normalization, using data standards (schema.org) & referential integrity (fk/pk) to ensure data is clean\n",
    "- How to speed up the underlying infrastructure with proper use of indexes\n",
    "- The importance of reproducible workflows to ingest data to ensure data is clean and well organized\n",
    "\n",
    "This will be our last class talking about relational databases.\n",
    "\n",
    "## WHAT is a Data Warehouse?\n",
    "\n",
    "- A Data Warehouse is a Data Management System\n",
    "- Data Analysts and BI Experts query a front end\n",
    "- Data is ingested in the back-end from multiple sources\n",
    "- A Data Warehouse simplifies data access and analysis.\n",
    "- Its indeed data stored in a database but in a denormalized fashion\n",
    "\n",
    "It's generally a process,\n",
    "\n",
    "<img src='img/warehouse.png' width='90%'>\n",
    "\n",
    "- Data moves from data sources to a data warehouse and specialized data marts.\n",
    "- Users access information from the data warehouse/data mart.\n",
    "- Users access data through applications.\n",
    "\n",
    "\n",
    "When building a data warehouse, we’re taking data from multiple sources and joining it in a comprehensive way to support multiple purposes, from reports and app-based analytics to data mining and machine learning applications.\n",
    "\n",
    "How do you think a data warehouse may look like?\n",
    "\n",
    "## General structure of Data Warehouse\n",
    "\n",
    "### Reciept story \n",
    "<img src='img/reciept.png' width='30%'>\n",
    "\n",
    "Image source: [wikiMedia](https://commons.wikimedia.org/w/index.php?curid=2612801)\n",
    "\n",
    "Fact Table: A table that contains the measurements of a business process of interest (in red).\n",
    "Dimension Table(s): Tables that provide context for the fact table (in violet). \n",
    "\n",
    "Let's look at a real world example,\n",
    "\n",
    "<img src='img/warehouseeg.png' width='90%'>\n",
    "\n",
    "Source: [ibm](https://www.ibm.com/docs/en/tap/3.5.3?topic=tables-example-fact-table-associated-dimensions),[oracle](https://docs.oracle.com/cd/E41507_01/epm91pbr3/eng/epm/penw/concept_MultidimensionalWarehouseMDW-9912e0.html)\n",
    "\n",
    "A Data Warehouse results from a process that takes data and transforms it into a “fact table” that a range of users can access. These facts are represented in a manner that is suited to analysis & use by a broad set of users. In addition, it powers up BI Applications like PowerBI or Tableau.\n",
    "\n",
    "## WHY \n",
    "\n",
    "- Data Can Be Complex\n",
    "- Multiple Sources\n",
    "- Multiple Uses\n",
    "- Multiple Users\n",
    "- Varied Skill Levels\n",
    "- Changing Data & Data Needs\n",
    "\n",
    "## Goals of Data Warehousing & BI\n",
    "\n",
    "The Data Warehouse system must:\n",
    "- Make information easily accessible\n",
    "- Present information consistently\n",
    "- Adapt to change\n",
    "- Present information in a timely way\n",
    "- Be secure\n",
    "- Be authoritative & trustworthy\n",
    "- Be accepted by end-users\n",
    "\n",
    "\n",
    "### So How Do We Meet Expectations?\n",
    "\n",
    "We have this dimensional model influenced by the business requirements & data realities.\n",
    "\n",
    "The Data Warehouse Design must be collaborative for the expectations to be met. Some parties include;\n",
    "- IT Managers\n",
    "- Executives\n",
    "- BI Analysts\n",
    "- Department Members\n",
    "- App Designers\n",
    "\n",
    "<img src='img/reality.png' width='80%'>\n",
    "\n",
    "## How Do We Build a Data Warehouse?\n",
    "\n",
    "You are already in the process of building an enterprise data warehouse. Here are the steps that you already did;\n",
    "- You identified the data sources\n",
    "- You performed data cleaning and normalization to load the data properly into the database\n",
    "- You are already doing some analysis on the data\n",
    "\n",
    "Even though you are already doing some joins to get to the analysis, we haven't properly identified the facts and dimensions for denormalizing the data to one flat table. So let's go through our Twitter data to see how this process looks like.\n",
    "\n",
    "### in our Twitter data, eg\n",
    "- What is the question?\n",
    "- Data we got in hand?\n",
    "- What Facts Do We Need?\n",
    "- What are Facts?  What are the Dimensions of those facts?\n",
    "\n",
    "A Data Warehouse is a type of database, but it conforms to a different data model. It's focused on a Fact Table with multiple dimensions.\n",
    "\n",
    "A Data Warehouse is part of a workflow that gets us from raw data (directly from a business process like sales, purchases, quality assessment metrics, real estate holdings, or some other measure) to a dataset easily queried and modeled as part of an analytics team.\n",
    "\n",
    "Thinking of our Twitter data, we had a central question that asked if daily cashtag volume was positively correlated to stock price changes. So, we took raw tweet data, stock symbol, and price data from multiple sources (Twitter, NYSE, Yahoo! Finance), came up with a data model and undertook data cleaning.\n",
    "\n",
    "If we're interested in understanding relationships between the tweets and changes in price, we might want to look at some different analyses.  For example:\n",
    "\n",
    "  * Sentiment analysis on tweets\n",
    "  * Presence of key terms (\"sell\" or \"buy\")\n",
    "  * Changes in price\n",
    "  * Analysis by individual ticker\n",
    "  * Aggregation at different scales (week, month)\n",
    "\n",
    "If we do that, we would want to work on a single table to undertake our analysis, so we're assured that there is consistency in our analysis.\n",
    "\n",
    "In lecture, I showed you the query we might use:\n",
    "\n",
    "```sql\n",
    "SELECT sy.nasdaqsymbol,\n",
    "\t   sy.securityname,\n",
    "       cl.tweet,\n",
    "\t   cl.userid,\n",
    "\t   CASE WHEN rp.replyid IS NULL THEN 'FALSE' \n",
    "\t                                ELSE 'TRUE' END AS reply,\n",
    "\t   CASE WHEN rt.retweetid IS NULL THEN 'FALSE' \n",
    "\t                                ELSE 'TRUE' END AS retweet,\n",
    "\t   ROUND(sv.open::numeric,2) AS open,\n",
    "\t   ROUND(sv.close::numeric, 2) AS close,\n",
    "\t   ROUND(sv.close::numeric, 2) - ROUND(sv.open::numeric,2) AS change,\n",
    "\t   date_trunc('day', cl.createdate)::date AS day\n",
    "FROM          tweets.cashtags AS ct\n",
    "INNER JOIN tweets.cleantweets AS cl ON cl.tweetid = ct.tweetid\n",
    "INNER JOIN     tweets.symbols AS sy ON sy.symbolid = ct.symbolid\n",
    "INNER JOIN tweets.stockvalues AS sv ON sv.symbolid = sy.symbolid AND sv.date = date_trunc('day', cl.createdate)::date\n",
    "LEFT JOIN      tweets.replies AS rp ON rp.tweetid = cl.tweetid\n",
    "LEFT JOIN     tweets.retweets AS rt ON rt.tweetid = cl.tweetid;\n",
    "```\n",
    "\n",
    "We're putting back the columns together and making some decisions about how we treat the data; for example, we're just adding a boolean (`TRUE`/`FALSE`) as to whether something is tweeted or retweeted (using the `CASE WHEN` statements).\n",
    "\n",
    "To create the wide table, given that this is essentially a derived table, we're going to use a `MATERIALIZED VIEW`.  This means that the output is committed to the disk, but it results from a query, so it's not something we're going to update directly.\n",
    "\n",
    "So our call would look something like this:\n",
    "\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW tweets.dtwide AS\n",
    "SELECT sy.nasdaqsymbol,\n",
    "\t   sy.securityname,\n",
    "       cl.tweet,\n",
    "\t   cl.userid,\n",
    "\t   CASE WHEN rp.replyid IS NULL THEN 'FALSE' \n",
    "\t                                ELSE 'TRUE' END AS reply,\n",
    "\t   CASE WHEN rt.retweetid IS NULL THEN 'FALSE' \n",
    "\t                                ELSE 'TRUE' END AS retweet,\n",
    "\t   ROUND(sv.open::numeric,2) AS open,\n",
    "\t   ROUND(sv.close::numeric, 2) AS close,\n",
    "\t   ROUND(sv.close::numeric, 2) - ROUND(sv.open::numeric,2) AS change,\n",
    "\t   date_trunc('day', cl.createdate)::date AS day\n",
    "FROM          tweets.cashtags AS ct\n",
    "INNER JOIN tweets.cleantweets AS cl ON  cl.tweetid = ct.tweetid\n",
    "INNER JOIN     tweets.symbols AS sy ON sy.symbolid = ct.symbolid\n",
    "INNER JOIN tweets.stockvalues AS sv ON sv.symbolid = sy.symbolid AND sv.date = date_trunc('day', cl.createdate)::date\n",
    "LEFT JOIN      tweets.replies AS rp ON  rp.tweetid = cl.tweetid\n",
    "LEFT JOIN     tweets.retweets AS rt ON  rt.tweetid = cl.tweetid;\n",
    "```\n",
    "\n",
    "I ran this already. Looking at the \"Materialized View\" tab in pgAdmin, you'll see the view there.  It's a table with three million rows, so we may want to add some indices to the table to speed up our query speed.\n",
    "\n",
    "The B-Tree index is most appropriate for most of the columns.  We may choose some other indexes for some of the other columns.  Let's take a look:\n",
    "The `nasdaqsymbol` column is simple, and we're unlikely to be searching for anything \n",
    "\n",
    "\n",
    "But let's pause for a minute and consider further what we're planning on doing with the data.  This will also help us plan out what indexes we might add to help speed things up.\n",
    "\n",
    "The B-Tree index is the default in Postgres and may be most appropriate for most columns. We may choose other indexes for some of the other columns and might even see other transformations we should apply to the data. Let's take a look:\n",
    "\n",
    "  * The `nasdaqsymbol` column could either use a B-TREE index or a HASH index. A HASH index is likely more appropriate if the ratio of unique symbols to total rows is very high (there are few unique symbols and many rows). However, remember that a hash index is only useful for equality. Are we likely to want to do analysis on stock symbols based on their letter configuration? (e.g., all stocks that start with *'A'*), or is it more likely that we're just looking at matches?\n",
    "\n",
    "```sql\n",
    "CREATE INDEX tweets.symbolhash_idx ON tweets.dtwide USING hash(nasdaqsymbol);\n",
    "CREATE INDEX tweets.symbolhash_idx ON tweets.dtwide USING btree(nasdaqsymbol);\n",
    "```\n",
    "\n",
    "  * `securityname` is something we might query, looking for things like *'Common Stock'* or *'ETF'* along with searches for the name of a company. In this case, full-text search may well be useful.  If we want to use full-text search, then, consulting the Postgres full-text search documentation, we might want to add a column that uses a `tsvector()` transformation of the `securityname` column.  We'll add it to our materialized view as the column security_ts and then work on an index:\n",
    "\n",
    "```sql\n",
    "CREATE INDEX securitysearch_idx ON tweets.dtwide USING GIN (security_ts);\n",
    "```\n",
    "\n",
    "This will allow us to use direct matching or full-text search to find the appropriate columns.\n",
    "\n",
    "  * `tweets` would be similar to the `security` name, so we'll add the same index and transformation.\n",
    "  * `reply` and `retweet` have only two values, *'TRUE'* and *'FALSE'*, so a hash index here is also likely useful.\n",
    "  * `open`, `close` and `change` are numeric values.  We'd likely be searching using statements like `open > 10`, or some sort of equality.  In that case, a `btree` index is likely sufficient.\n",
    "  * `day` is interesting.  If the dataset is not ordered by `day`, we could use a `btree` index.  If we were only searching by exact matches, a `hash` might be useful here.  **IF HOWEVER** we order our `MATERIALIZED VIEW` by `day`, then we can use a `brin` index.  The `brin` index assigns values to ordered address blocks.\n",
    "  \n",
    "So, this leaves us with some extra transformations and a set of indexes to apply:\n",
    "\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW tweets.dtwide AS\n",
    "SELECT sy.nasdaqsymbol,\n",
    "\t   sy.securityname,\n",
    "       to_tsvector(sy.securityname) AS security_ts,\n",
    "       cl.tweet,\n",
    "       to_tsvector(cl.tweet) AS tweet_ts,\n",
    "\t   cl.userid,\n",
    "\t   CASE WHEN rp.replyid IS NULL THEN 'FALSE' \n",
    "\t                                ELSE 'TRUE' END AS reply,\n",
    "\t   CASE WHEN rt.retweetid IS NULL THEN 'FALSE' \n",
    "\t                                ELSE 'TRUE' END AS retweet,\n",
    "\t   ROUND(sv.open::numeric,2) AS open,\n",
    "\t   ROUND(sv.close::numeric, 2) AS close,\n",
    "\t   ROUND(sv.close::numeric, 2) - ROUND(sv.open::numeric,2) AS change,\n",
    "\t   date_trunc('day', cl.createdate)::date AS day\n",
    "FROM          tweets.cashtags AS ct\n",
    "INNER JOIN tweets.cleantweets AS cl ON  cl.tweetid = ct.tweetid\n",
    "INNER JOIN     tweets.symbols AS sy ON sy.symbolid = ct.symbolid\n",
    "INNER JOIN tweets.stockvalues AS sv ON sv.symbolid = sy.symbolid AND sv.date = date_trunc('day', cl.createdate)::date\n",
    "LEFT JOIN      tweets.replies AS rp ON  rp.tweetid = cl.tweetid\n",
    "LEFT JOIN     tweets.retweets AS rt ON  rt.tweetid = cl.tweetid\n",
    "ORDER BY date_trunc('day', cl.createdate)::date ASC;\n",
    "```\n",
    "\n",
    "And then add the following indices:\n",
    "```sql\n",
    "CREATE INDEX  symbol_idx ON tweets.dtwide USING GIN(security_ts);\n",
    "CREATE INDEX   tweet_idx ON tweets.dtwide USING GIN(tweet_ts);\n",
    "CREATE INDEX  nasdaq_idx ON tweets.dtwide USING btree(securityname);\n",
    "CREATE INDEX  userid_idx ON tweets.dtwide USING hash(userid);\n",
    "CREATE INDEX   reply_idx ON tweets.dtwide USING hash(reply);\n",
    "CREATE INDEX retweet_idx ON tweets.dtwide USING hash(retweet);\n",
    "CREATE INDEX    open_idx ON tweets.dtwide USING btree(open);\n",
    "CREATE INDEX   close_idx ON tweets.dtwide USING btree(close);\n",
    "CREATE INDEX  change_idx ON tweets.dtwide USING btree(change);\n",
    "CREATE INDEX     day_idx ON tweets.dtwide USING BRIN(day);\n",
    "```\n",
    "\n",
    "We can directly query our own mini-data warehouse using simple SQL queries given these changes and new indices.  For example, \n",
    "\n",
    "```sql\n",
    "SELECT symbol, ROUND(change / open, 2), COUNT(*) AS tweets\n",
    "FROM tweets.dtwide\n",
    "GROUP BY day, symbol, change;\n",
    "```\n",
    "\n",
    "It would give us the relationship between the number of tweets in a day and the proportional change in stock value.  All of this work then allows us to simplify our workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42da4150-20dc-46b7-ad19-838ed4da16c5",
   "metadata": {},
   "source": [
    "```\n",
    "import os\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "conString = {'host':os.environ.get('DB_HOST'),\n",
    "             'dbname':os.environ.get('DB_NAME'),\n",
    "             'user':os.environ.get('DB_USER'),\n",
    "             'password':os.environ.get('DB_PASS'),\n",
    "             'port':os.environ.get('DB_PORT')}\n",
    "\n",
    "conn = psycopg2.connect(**conString)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"SELECT nasdaqsymbol, ROUND(change / open, 2), COUNT(*) AS tweets\n",
    "               FROM tweets.dtwide\n",
    "               GROUP BY day, nasdaqsymbol, change, open;\"\"\")\n",
    "\n",
    "summarized = pd.DataFrame(cur.fetchall(), columns=['nasdaqsymbol', 'change', 'tweets'])\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(summarized, \n",
    "              x='tweets',\n",
    "              y='change',\n",
    "              title='Stock value change by tweet count.', \n",
    "              hover_name='nasdaqsymbol',)\n",
    "cur.close()\n",
    "fig.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b7bfe",
   "metadata": {},
   "source": [
    "## Differences \n",
    "### DW vs RMDBS\n",
    "\n",
    "| Database                                                                               | Data Warehouse                           |\n",
    "|----------------------------------------------------------------------------------------|------------------------------------------|\n",
    "| Concerned with data security and durability                                            | Concerned with broad access for analysis |\n",
    "| Highly normalized to ensure referential integrity & reduce update/insertion anomalies. | Denormalized to ensure ease of access    |\n",
    "| Designed to support data ingestion.                                                    | they are only updated by the system      |\n",
    "| Data can go in and out                                                                 | User data goes out, not in               |\n",
    "\n",
    "### ETL vs ELT\n",
    "\n",
    "| ETL                                            | ELT                                    |\n",
    "|------------------------------------------------|----------------------------------------|\n",
    "| Extract, Transform and Load                    | Extract, Load and Transform            |\n",
    "| Data is loaded after performing transformation | Data first loaded and then transformed |\n",
    "| Mostly used for structured data                | Mostly used for unstructured data      |\n",
    "| SQL based databases                            | NoSQL based systems                    |\n",
    "| For building Data Warehouses                   | For building data lakes                |\n",
    "\n",
    "### OLAP vs OLTP\n",
    "\n",
    "| OLAP                                          | OLTP                                  |\n",
    "|-----------------------------------------------|---------------------------------------|\n",
    "| Online analytical processing                  | Online transactional processing       |\n",
    "| Focus on Insert, Update, Delete for databases | Focuses on extract data for analyzing |\n",
    "| uses the data warehouse                       | uses the database                     |\n",
    "\n",
    "### BI vs DW\n",
    "\n",
    "Data warehouse powers up BI. (Business intelligence)\n",
    "\n",
    "## Wrap up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
