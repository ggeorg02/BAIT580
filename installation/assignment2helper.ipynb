{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81dc6dbc-00b9-40de-9d2e-b7e87172ddee",
   "metadata": {},
   "source": [
    "# Assignment 2 helper \n",
    "\n",
    "As part of assignment 2, you will be going through a guided process of creating a workflow by answering some of the questions with our tweets data. \n",
    "\n",
    "This assignment is like a mini version of the workflow you will produce as part of your final project. In your final project assignment, you need to include some other useful concepts like indexing and warehousing solutions wherever appropriate. You can also check out in lecture 6 how warehousing solutions are applied to the current problem. We will be seeing how indexing and warehousing solutions apply to our assignment dataset in lecture 6.\n",
    "\n",
    "Basic knowledge in SQL and python is necessary for going through these assignments. This document is written to help some students who need additional support with the SQL needed for this assignment. You will be using mainly SQL commands that you are familiar with, but one reason some students get overwhelmed by seeing this assignment is probably this could be the first time you use SQL for real in a workflow. That is why the questions are not straightforward like in a regular assignment. \n",
    "\n",
    "Colby will give another session on basic python needed during his office hours.\n",
    "\n",
    "The idea of dealing with things in the cloud is that you don't want to worry about any of the installation processes in your laptop and can spin a database and bring it down whenever you want. Also, you all got 100 credits, so make sure you use it, so it's okay to spin a bigger instance if you want to get a faster experience. However, remember that bigger instances cost more, so better shut it down when you don't use it.\n",
    "\n",
    "It's your choice to spin a different database instance for this assignment or use the previous instance. However, I strongly suggest you use a separate instance for your project.\n",
    "\n",
    "The process of setting it up will only take a minute from your end, and from the AWS end, it might take around 10 minutes for it to be available. [I am putting a small video on doing it.](https://ubc.zoom.us/rec/play/a6qhchUCkFCSs-7aT1zeMDWG0Gsu2RK1Gj66NZpNhXvWPqxQBtQ353cCdeP9kMy9CQXFvlaOTxGYTjfv.t7YkUbcpCSYe1MOL?startTime=1642465726000)\n",
    "\n",
    "First, we want to load the data into the database. So why are we loading dump again? So last time, we put the raw data into our database. After that, I did a lot of data cleaning to normalize it for our analysis. [You can check this notebook](https://canvas.ubc.ca/files/18519982/download?download_frd=1)  to see how I did the process of cleaning. But you don't want to run this notebook for doing your assignment, it's just put here for your reference, but you can use it for some help when dealing with group assignments.\n",
    "\n",
    "The cleaned data is loaded to the tweets schema, and I took a dump of the tweets schema. Next, we want to load this data into our database. \n",
    "\n",
    "Hopefully, you all are comfortable uploading dumps from our previous communications. However, [I created this video showing how to upload a dump](https://ubc.zoom.us/rec/share/MzaxQiS4FmDqw2JUipi6PB6y3llcy5Y7-wAtTqfFIZhzdJfQ54V9qK4fv18nH8wD.Q9iSG0syxrBgTuuF?startTime=1642468195000\n",
    "), it will only take ~3 minutes from your end.\n",
    "\n",
    "Later you can check your schema and tables in your pgadmin. [Check out this small video.](https://ubc.zoom.us/rec/play/qDHX9a2cmv1Sc0NTrCiA4YauF2NXS_E162eTqom12yJQbEdPvzq2D4_6RYI8d7IcBuTiECaUrDRUSTA4.xk4_nplwVjqguXI4?startTime=1642468736000) \n",
    "\n",
    "Passcode for above videos check [in slack](https://bait-580.slack.com/archives/C02SU3M63S5/p1642528551003000).\n",
    "\n",
    "If you want any help with the .env file creation, you [can check out the first few minutes of this office hour](https://ubc.zoom.us/rec/share/DovyeJvun1E3Sb06bXAuQS_GZbwlTbJktsUcqZxHVs4ekFuCV8cfEzpm4O86l38.D3b10SWO9QllKx_y), and we are setting it up both in windows and mac.\n",
    "\n",
    "Check out the course channel for access codes for the above videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78300c06-fff0-466f-9523-97f625df3312",
   "metadata": {},
   "source": [
    "```\n",
    "# Put your code here.  More than likely you can copy/paste right in.\n",
    "# Make sure you have your .env file in the same folder as this project file.\n",
    "\n",
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import numpy\n",
    "import sparklines\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "conString = {'host':os.environ.get('DB_HOST'),\n",
    "             'dbname':os.environ.get('DB_NAME'),\n",
    "             'user':os.environ.get('DB_USER'),\n",
    "             'password':os.environ.get('DB_PASS'),\n",
    "             'port':os.environ.get('DB_PORT')}\n",
    "print(conString)\n",
    "\n",
    "conn = psycopg2.connect(**conString)\n",
    "cur = conn.cursor()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002a85c-27dc-4a98-bed3-c7bc70e91504",
   "metadata": {},
   "source": [
    "SQL commands you need to go through this assignment include some fundamental ones.\n",
    "\n",
    "- SELECT\n",
    "- LIMIT\n",
    "- DISTINCT\n",
    "- COUNT\n",
    "- INNER JOIN\n",
    "- GROUP BY\n",
    "- ORDER BY\n",
    "- WHERE\n",
    "\n",
    "Some commands or inbuilt functions that are not too common include\n",
    "- string functions- LTRIM and regex_matches\n",
    "- TO_DATE\n",
    "- WITH IN combination Queries\n",
    "- date_trunc\n",
    "\n",
    "To some of these not-so-common commands, I put instructions and references in the assignment to look it up and use it. However, here I will go through all of these not-so-common commands in explaining why we use them generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c35e93-f957-4087-a3bf-c8e049779b52",
   "metadata": {},
   "source": [
    "```\n",
    "## creating a dummy table \n",
    "query = \"\"\"DROP TABLE IF EXISTS import.example;\n",
    "CREATE TABLE import.example (id text, date text, sentence text);\n",
    "INSERT INTO import.example(id, date,sentence) VALUES('111','Thu May 18 22:00:00 +0000 2017','This is a test sentence $NEW $OLD $small');\n",
    "INSERT INTO import.example(id, date,sentence) VALUES('222','Fri Jun 20 09:00:00 +0000 2017','Testing sentence $MBAN');\n",
    "INSERT INTO import.example(id, date,sentence) VALUES('333','Mon Jul 18 21:00:00 +0000 2017','Sentence $XXX');\n",
    "INSERT INTO import.example(id, date,sentence) VALUES('444','Mon Jul 18 14:00:00 +0000 2017','Sentence different type $XXX');\n",
    "INSERT INTO import.example(id, date,sentence) VALUES('555','Thu May 18 14:00:00 +0000 2017','I am tweeting again $OLD');\n",
    "INSERT INTO import.example(id, date,sentence) VALUES('666','Fri Jun 20 16:00:00 +0000 2017','Sentence different type $MBAN');\n",
    "\"\"\"\n",
    "cur.execute(query)\n",
    "conn.commit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd19d0-4657-4895-8d75-5da3e5960916",
   "metadata": {},
   "source": [
    "## regexp_matches\n",
    "\n",
    "regexp_matches are helpful when we want to generate some valuable text from a text column. In this dummy example, it's whatever that follows $. Of course, you can check the official docs linked-to assignment, but you don't need all that you can probably check out [this](https://www.postgresqltutorial.com/postgresql-regexp_matches/), as this explains just what we want. So please don't worry too much about regex.\n",
    "\n",
    "Basic syntax is \n",
    "REGEXP_MATCHES(source_string, pattern [, flags])\n",
    "\n",
    "- source_string: here will be the text column value\n",
    "- pattern: The regex pattern\n",
    "- flags: it's optional, but we will be giving flag 'g' that search globally for each occurrence.\n",
    "\n",
    "The regex pattern is a different study area and can be very useful in many situations. But you can use this [regex generator](https://regex-generator.olafneumann.org/?sampleText=2020-03-12T13%3A34%3A56.123Z%20INFO%20%20%5Borg.example.Class%5D%3A%20This%20is%20a%20%23simple%20%23logline%20containing%20a%20%27value%27.&flags=i&onlyPatterns=false&matchWholeLine=false&selection=) for coming up with your pattern.\n",
    "\n",
    "Here is how it behaves in our dummy example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc835f-4e49-4348-b1d3-cba8be20497f",
   "metadata": {},
   "source": [
    "```\n",
    "query=\"\"\"SELECT eg.id, \n",
    "  regexp_matches(eg.sentence, '\\$[A-Z]+', 'g')\n",
    "FROM import.example AS eg;\"\"\"\n",
    "cur.execute(query)\n",
    "cur.fetchall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdfc88d-40e2-40fc-9110-ceb44f7c8ea5",
   "metadata": {},
   "source": [
    "# why we use unnest ? \n",
    "unnest just opens up the array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95dfa1f-f6d5-41d2-8225-d22db19df708",
   "metadata": {},
   "source": [
    "```\n",
    "cur = conn.cursor()\n",
    "query=\"\"\"SELECT eg.id, \n",
    "  unnest(regexp_matches(eg.sentence, '\\$[A-Z]+', 'g')) AS substring\n",
    "FROM import.example AS eg;\"\"\"\n",
    "cur.execute(query)\n",
    "cur.fetchall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422b226-24a9-45b7-8964-1f68b2b199fd",
   "metadata": {},
   "source": [
    "# LTRIM\n",
    "Syntax:\n",
    "    LTRIM(string,trimming_text)\n",
    "It takes out the leading character from the string. There is also RTRIM and BTRIM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a284e5-e5d6-4bdb-9cb2-425dddf542db",
   "metadata": {},
   "source": [
    "```\n",
    "conn.rollback()\n",
    "cur = conn.cursor()\n",
    "query=\"\"\"SELECT eg.id, \n",
    "  LTRIM(unnest(regexp_matches(eg.sentence, '\\$[A-Z]+', 'g')),'$') AS substring\n",
    "FROM import.example AS eg LIMIT 10;\"\"\"\n",
    "cur.execute(query)\n",
    "cur.fetchall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b0da7-d907-435e-a827-dd72bc5b4076",
   "metadata": {},
   "source": [
    "## Constructing a dataframe from query returned.\n",
    "As we mentioned in our 3rd lecture, psycopg2 returns the data as a list of tuples. The best thing you can do to work with these tuples is to convert them into pandas dataframe. After that, you are in the python world to deal with any further transformation before getting to the visualization. You might need to do this for Question 5a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d51887-9ab0-4979-911c-e05e1a353c2c",
   "metadata": {},
   "source": [
    "```\n",
    "# rebuilt to a dataframe \n",
    "cur.execute(query)\n",
    "stocktweets = cur.fetchall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef1741-3058-4b51-8797-acb1e5550cf4",
   "metadata": {},
   "source": [
    "```\n",
    "##as you see list of tuples \n",
    "print(stocktweets)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100e2ea-728b-4ea2-9aa2-7156f68aa6d8",
   "metadata": {},
   "source": [
    "```\n",
    "# rebuilt to a dataframe \n",
    "stockdf = pd.DataFrame(stocktweets, columns=['id', 'substring'])\n",
    "# Now limit it to three stock tweets:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab64187-ac23-4400-8d66-8906c19edb83",
   "metadata": {},
   "source": [
    "```\n",
    "stockdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c995678-b0ef-4543-a1c2-7c99709569af",
   "metadata": {},
   "source": [
    "Let's move on to do some data transformations. As mentioned in lecture4, we need to be very careful when dealing with dates as there are a wide variety of dates formats out there, and we need to make sure that it is appropriately aligned with all the datasets. I have already converted the Twitter table to the clean tweets table, and you will be using that from question 3b onwards, but for getting a feel of the conversion, we will use 3a to convert a column to date type.\n",
    "\n",
    "TO_DATE - You can use this to convert a text to a date format. Here you specify the format your date is in and convert it to the date format. WHY convert to date format? Because if we convert to a date format, we can apply a variety of [date time operations.](https://www.postgresql.org/docs/9.1/functions-datetime.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91636d8f-8a3e-4e3b-a930-74f622690a44",
   "metadata": {},
   "source": [
    "```\n",
    "query = \"\"\"SELECT TO_DATE(date, 'Dy Mon DD HH24:MI:SS +0000 YYYY') AS date\n",
    "           FROM import.example AS tw\"\"\"\n",
    "cur.execute(query)\n",
    "cur.fetchall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e0963-9e0e-4a05-bd0a-95299c0b6dbc",
   "metadata": {},
   "source": [
    "Below cell is not required for your assignment, but I am creating a column named datacol with timestamp type,so that I can demonstrate how we can apply `date_trunc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ee36c-32ca-4802-9a1c-685ca06e7540",
   "metadata": {},
   "source": [
    "```\n",
    "query = \"\"\"DROP TABLE import.exampleclean;\n",
    "CREATE TABLE import.exampleclean AS \n",
    "SELECT id,LTRIM(unnest(regexp_matches(eg.sentence, '\\$[A-Z]+', 'g')),'$') AS instahash,TO_TIMESTAMP(eg.date, 'Dy Mon DD HH24:MI:SS +0000 YYYY') as datecol\n",
    "FROM import.example as eg;\"\"\"\n",
    "cur.execute(query)\n",
    "conn.commit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4827fb5-d411-49bd-80e9-3b6d0797a6d7",
   "metadata": {},
   "source": [
    "About date_trunc, I came across an amazing article [here](https://mode.com/blog/date-trunc-sql-timestamp-function-count-on/), and hence not writing about it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7879ec7-aaa2-4992-aac7-2e3c1603f711",
   "metadata": {},
   "source": [
    "```\n",
    "query = \"\"\"SELECT instahash,date_trunc('day',datecol),COUNT(*) AS total\n",
    "FROM import.exampleclean\n",
    "GROUP BY instahash,date_trunc('day',datecol)\"\"\"\n",
    "cur.execute(query)\n",
    "cur.fetchall()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fe1ca-3c0d-4bda-ae07-019373e560d6",
   "metadata": {},
   "source": [
    "### Using WITH AS and IN\n",
    "This combination is beneficial in many places. You can read about it [here](https://www.postgresql.org/docs/9.1/queries-with.html). You can think of it as creating temporary tables by querying another table. For example, here we are using the following query ...\n",
    "\n",
    "```sql\n",
    "SELECT instahash,date_trunc('day',datecol),COUNT(*) AS total\n",
    "FROM import.exampleclean\n",
    "GROUP BY instahash,date_trunc('day',datecol) LIMIT 1\n",
    "```\n",
    "\n",
    "...and capturing the result to a kind of temporary table called `random`. This `random` is used to query in another SQL query. Probably you learned about it, but check [this out](https://www.techonthenet.com/postgresql/in.php) to know more.\n",
    "\n",
    "```sql\n",
    "select * from import.exampleclean\n",
    "where instahash IN (select instahash from random)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c0000-66dd-4539-bb99-1b67292d1598",
   "metadata": {},
   "source": [
    "```\n",
    "## https://www.postgresql.org/docs/9.1/queries-with.html\n",
    "query=\"\"\"WITH random as (SELECT * FROM import.exampleclean LIMIT 1)\n",
    "select * from import.exampleclean\n",
    "where instahash IN (select instahash from random)\"\"\"\n",
    "cur.execute(query)\n",
    "cur.fetchall()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
